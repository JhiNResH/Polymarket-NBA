import pandas as pd
import os
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def merge_and_convert():
    """åˆä½µèˆŠæ ¼å¼å’Œæ–°æ ¼å¼çš„ CSV æ•¸æ“š"""

    backup_file = 'polymarket_bets_backup_20260113_143232.csv'

    if not os.path.exists(backup_file):
        logger.error("âŒ æ‰¾ä¸åˆ°å‚™ä»½æª”æ¡ˆ")
        return

    # æ‰‹å‹•è§£ææ··åˆæ ¼å¼çš„æª”æ¡ˆ
    old_format_data = []
    new_format_data = []

    with open(backup_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # è·³éæ¨™é¡Œå’Œç©ºè¡Œ
    for line in lines[2:]:
        line = line.strip()
        if not line:
            continue

        fields = [f.strip() for f in line.split(',')]

        # åˆ¤æ–·æ˜¯èˆŠæ ¼å¼ (8 æ¬„) é‚„æ˜¯æ–°æ ¼å¼ (14 æ¬„)
        if len(fields) == 8:
            # èˆŠæ ¼å¼: Date, Match, Pick, Price, Prob, Stake, Result, Profit
            try:
                old_format_data.append({
                    'Date': fields[0],
                    'Match': fields[1],
                    'Pick': fields[2],
                    'Price': float(fields[3]) if fields[3] else 0,
                    'Prob': float(fields[4]) if fields[4] else 0,
                    'Stake': float(fields[5]) if fields[5] else 0,
                    'Result': fields[6],
                    'Profit': fields[7]
                })
            except ValueError:
                continue

        elif len(fields) >= 14:
            # æ–°æ ¼å¼: Date, Sport, Match, Pick, Poly_Price, Implied_Odds, True_Prob, EV, Kelly_Stake, Expected_Profit, Match_Score, Link, Result, Actual_Profit
            try:
                new_format_data.append({
                    'Date': fields[0],
                    'Sport': fields[1],
                    'Match': fields[2],
                    'Pick': fields[3],
                    'Poly_Price': float(fields[4]) if fields[4] else 0,
                    'Implied_Odds': float(fields[5]) if fields[5] else 0,
                    'True_Prob': float(fields[6]) if fields[6] else 0,
                    'EV': float(fields[7]) if fields[7] else 0,
                    'Kelly_Stake': float(fields[8]) if fields[8] else 0,
                    'Expected_Profit': float(fields[9]) if fields[9] else 0,
                    'Match_Score': float(fields[10]) if fields[10] else 0,
                    'Link': fields[11],
                    'Result': fields[12] if len(fields) > 12 else '',
                    'Actual_Profit': fields[13] if len(fields) > 13 else ''
                })
            except ValueError:
                continue

    logger.info(f"ğŸ“Š æ‰¾åˆ° {len(old_format_data)} ç­†èˆŠæ ¼å¼æ•¸æ“š")
    logger.info(f"ğŸ“Š æ‰¾åˆ° {len(new_format_data)} ç­†æ–°æ ¼å¼æ•¸æ“š")

    # è½‰æ›èˆŠæ ¼å¼åˆ°æ–°æ ¼å¼
    converted_data = []
    for old in old_format_data:
        converted_data.append({
            'Date': old['Date'],
            'Sport': 'Unknown',
            'Match': old['Match'],
            'Pick': old['Pick'],
            'Poly_Price': old['Price'],
            'Implied_Odds': round(1 / old['Price'], 2) if old['Price'] > 0 else 0,
            'True_Prob': old['Prob'],
            'EV': round((old['Prob'] - old['Price']) / old['Price'], 4) if old['Price'] > 0 else 0,
            'Kelly_Stake': old['Stake'],
            'Expected_Profit': 0.0,
            'Match_Score': 100.0,
            'Link': '',
            'Result': old['Result'],
            'Actual_Profit': old['Profit']
        })

    # åˆä½µæ‰€æœ‰æ•¸æ“š
    all_data = converted_data + new_format_data

    # å‰µå»º DataFrame
    df = pd.DataFrame(all_data)

    # ç§»é™¤é‡è¤‡é …ï¼ˆåŸºæ–¼ Date, Match, Pickï¼‰
    df_unique = df.drop_duplicates(subset=['Date', 'Match', 'Pick'], keep='last')

    logger.info(f"ğŸ“Š åˆä½µå¾Œå…± {len(df_unique)} ç­†å”¯ä¸€æ•¸æ“š")

    # å„²å­˜ç‚ºæ–°æ ¼å¼ CSV
    output_file = 'polymarket_bets.csv'
    df_unique.to_csv(output_file, index=False)

    logger.info(f"âœ… å·²å„²å­˜è‡³ {output_file}")

    # é¡¯ç¤ºå‰å¹¾ç­†
    logger.info("\nğŸ“‹ å‰ 3 ç­†æ•¸æ“š:")
    print(df_unique.head(3).to_string(index=False))

    logger.info(f"\nğŸ“‹ æ¬„ä½åˆ—è¡¨:")
    logger.info(f"   {df_unique.columns.tolist()}")

if __name__ == "__main__":
    logger.info("ğŸ”„ é–‹å§‹åˆä½µ CSV æ•¸æ“š...\n")
    merge_and_convert()
    logger.info("\nâœ… å®Œæˆï¼ç¾åœ¨å¯ä»¥åŸ·è¡Œ google_sheets_sync.py")
