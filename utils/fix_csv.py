import pandas as pd
import os
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def fix_csv():
    """ä¿®å¾© CSV æª”æ¡ˆæ ¼å¼å•é¡Œ"""
    csv_file = 'polymarket_bets.csv'

    if not os.path.exists(csv_file):
        logger.info("âœ… CSV æª”æ¡ˆä¸å­˜åœ¨ï¼Œç„¡éœ€ä¿®å¾©")
        return

    # å‚™ä»½åŸå§‹æª”æ¡ˆ
    backup_file = f'polymarket_bets_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
    os.rename(csv_file, backup_file)
    logger.info(f"ğŸ“¦ å·²å‚™ä»½åŸå§‹æª”æ¡ˆè‡³: {backup_file}")

    # å˜—è©¦è®€å–ä¸¦ä¿®å¾©
    try:
        # å…ˆå˜—è©¦é€è¡Œè®€å–ï¼Œæ‰¾å‡ºå•é¡Œè¡Œ
        with open(backup_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        logger.info(f"ğŸ“„ åŸå§‹æª”æ¡ˆå…± {len(lines)} è¡Œ")

        # åˆ†ææ¨™é¡Œè¡Œ
        if lines:
            header = lines[0].strip().split(',')
            logger.info(f"ğŸ“‹ æ¨™é¡Œæ¬„ä½ ({len(header)} å€‹): {header}")

        # æª¢æŸ¥æ¯ä¸€è¡Œ
        valid_lines = [lines[0]]  # ä¿ç•™æ¨™é¡Œè¡Œ
        problematic_lines = []

        for i, line in enumerate(lines[1:], start=2):
            fields = line.strip().split(',')
            if len(fields) == len(header):
                valid_lines.append(line)
            else:
                problematic_lines.append((i, len(fields), line))
                logger.warning(f"âš ï¸ ç¬¬ {i} è¡Œæ¬„ä½æ•¸ä¸ç¬¦: é æœŸ {len(header)}ï¼Œå¯¦éš› {len(fields)}")

        # å¯«å…¥ä¿®å¾©å¾Œçš„æª”æ¡ˆ
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.writelines(valid_lines)

        logger.info(f"âœ… ä¿®å¾©å®Œæˆï¼")
        logger.info(f"   - ä¿ç•™æœ‰æ•ˆè¡Œ: {len(valid_lines) - 1}")
        logger.info(f"   - ç§»é™¤å•é¡Œè¡Œ: {len(problematic_lines)}")

        if problematic_lines:
            logger.info(f"\nâŒ å•é¡Œè¡Œè©³æƒ…:")
            for line_num, field_count, content in problematic_lines[:5]:  # åªé¡¯ç¤ºå‰ 5 è¡Œ
                logger.info(f"   ç¬¬ {line_num} è¡Œ ({field_count} å€‹æ¬„ä½): {content[:100]}...")

        # é©—è­‰ä¿®å¾©çµæœ
        df = pd.read_csv(csv_file)
        logger.info(f"\nğŸ“Š ä¿®å¾©å¾Œçš„ CSV:")
        logger.info(f"   - æ¬„ä½: {df.columns.tolist()}")
        logger.info(f"   - è³‡æ–™ç­†æ•¸: {len(df)}")

        # é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™
        if not df.empty:
            logger.info(f"\nğŸ“‹ å‰ 3 ç­†è³‡æ–™:")
            print(df.head(3).to_string())

    except Exception as e:
        logger.error(f"âŒ ä¿®å¾©å¤±æ•—: {e}")
        # é‚„åŸå‚™ä»½
        if os.path.exists(backup_file):
            os.rename(backup_file, csv_file)
            logger.info("â†©ï¸  å·²é‚„åŸåŸå§‹æª”æ¡ˆ")

def migrate_old_csv():
    """å°‡èˆŠæ ¼å¼ CSV é·ç§»åˆ°æ–°æ ¼å¼"""
    csv_file = 'polymarket_bets.csv'

    if not os.path.exists(csv_file):
        logger.info("âœ… CSV æª”æ¡ˆä¸å­˜åœ¨ï¼Œç„¡éœ€é·ç§»")
        return

    try:
        # å˜—è©¦è®€å–èˆŠæ ¼å¼
        df = pd.read_csv(csv_file, on_bad_lines='skip')

        # æª¢æŸ¥æ˜¯å¦ç‚ºèˆŠæ ¼å¼ï¼ˆ8 å€‹æ¬„ä½ï¼‰
        old_columns = ['Date', 'Match', 'Pick', 'Price', 'Prob', 'Stake', 'Result', 'Profit']

        if list(df.columns) == old_columns:
            logger.info("ğŸ”„ åµæ¸¬åˆ°èˆŠæ ¼å¼ CSVï¼Œæ­£åœ¨é·ç§»...")

            # å‰µå»ºæ–°æ ¼å¼
            new_df = pd.DataFrame({
                'Date': df['Date'],
                'Sport': 'Unknown',  # èˆŠæ ¼å¼æ²’æœ‰æ­¤æ¬„ä½
                'Match': df['Match'],
                'Pick': df['Pick'],
                'Poly_Price': df['Price'],
                'Implied_Odds': 1 / df['Price'],  # è¨ˆç®—éš±å«è³ ç‡
                'True_Prob': df['Prob'],
                'EV': 0.0,  # èˆŠæ ¼å¼æ²’æœ‰æ­¤æ¬„ä½
                'Kelly_Stake': df['Stake'],
                'Expected_Profit': 0.0,  # èˆŠæ ¼å¼æ²’æœ‰æ­¤æ¬„ä½
                'Match_Score': 100.0,  # å‡è¨­èˆŠæ•¸æ“šåŒ¹é…åˆ†æ•¸ç‚º 100
                'Link': '',  # èˆŠæ ¼å¼æ²’æœ‰æ­¤æ¬„ä½
                'Result': df['Result'],
                'Actual_Profit': df['Profit']
            })

            # å‚™ä»½ä¸¦å¯«å…¥
            backup_file = f'polymarket_bets_old_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
            os.rename(csv_file, backup_file)
            logger.info(f"ğŸ“¦ å·²å‚™ä»½èˆŠæª”æ¡ˆè‡³: {backup_file}")

            new_df.to_csv(csv_file, index=False)
            logger.info(f"âœ… é·ç§»å®Œæˆï¼å·²æ›´æ–°ç‚ºæ–°æ ¼å¼")

        else:
            logger.info("âœ… å·²æ˜¯æ–°æ ¼å¼ï¼Œç„¡éœ€é·ç§»")

    except Exception as e:
        logger.error(f"âŒ é·ç§»å¤±æ•—: {e}")

if __name__ == "__main__":
    logger.info("ğŸ”§ é–‹å§‹ä¿®å¾© CSV æª”æ¡ˆ...\n")

    # æ­¥é©Ÿ 1: å˜—è©¦é·ç§»èˆŠæ ¼å¼
    migrate_old_csv()

    # æ­¥é©Ÿ 2: ä¿®å¾©æ ¼å¼å•é¡Œ
    fix_csv()

    logger.info("\nâœ… æ‰€æœ‰ä¿®å¾©å®Œæˆï¼ç¾åœ¨å¯ä»¥åŸ·è¡Œ google_sheets_sync.py")
